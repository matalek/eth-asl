% % % % % % % % % % % % % % %
\documentclass[11pt]{article}
\usepackage[a4paper, portrait, margin=1in]{geometry}
% % % % % % % % % % % % % % %


\usepackage{fancyhdr}
\usepackage{siunitx}
\usepackage{amsmath}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

\usepackage{array}
\newcolumntype{$}{>{\global\let\currentrowstyle\relax}}
\newcolumntype{^}{>{\currentrowstyle}}
\newcommand{\rowstyle}[1]{\gdef\currentrowstyle{#1}%
  #1\ignorespaces
}


\usepackage{array}
\usepackage{booktabs}

\fancypagestyle{firstpagefooter}
{
\lfoot{Version: 25.11.2016}
\cfoot{}
\rfoot{\thepage}
}
\cfoot{\thepage}


\begin{document}

\title{Advanced Systems Lab (Fall'16) -- Third Milestone}

\author{Name: \emph{Aleksander Matusiak}\\Legi number: \emph{16-931-925}}

\date{
\vspace{4cm}
\textbf{Grading} \\
\begin{tabular}{|c|c|}
\hline  \textbf{Section} & \textbf{Points} \\ 
\hline  1 &  \\ 
\hline  2 &  \\ 
\hline  3 &  \\ 
\hline  4 &  \\ 
\hline  5 &  \\ 
\hline \hline Total & \\
\hline 
\end{tabular} 
}

\maketitle
\thispagestyle{firstpagefooter}
\newpage

\section*{Notes on writing the report}

The report does not need to be extensive but it must be concise, complete, and correct. Conciseness is important  in  terms  of  content  and explanations,  focusing  on  what  has  been  done and  explanations  of the results. A long report is not necessarily a better report, especially if there are aspects that  remain  unexplained.  Completeness  implies  that  the  report  should  give  a comprehensive idea of what has been done by mentioning all key aspects of the modeling and analysis effort. Limited analysis because of flaws in the system or lack of experimental data from Milestones 1 or 2 are not  valid  arguments  for  an incomplete  report.  If  bugs  or  lack  of  data  prevent  you  from  doing  a  correct analysis, the system must be debugged and new data collected. In case the system has been modified, include a short description of the changes as an appendix.

Remember  that  this is  a  report  about modeling  and  analyzing the  system you  have  designed  and  built, using  the experimental data you have collected. There is no unique way to do the report and you may choose  to  focus  on  different  aspects  of  the  system  as  long  as  you deliver a  complete analysis of  its behavior. Keep in mind that, \emph{for all queuing models in the report}, you need to explain how the the parameters of the model were determined and from which experiments the data comes from (adding a reference to the exact graph, table, etc. from the previous milestones). You have to find all system metrics that can be derived using the corresponding formulas and then match to the experimental results, explaining the similarities and differences in quantitative and qualitative terms. The calculations and the numbers you derive might need to be explained with references to the logs and sources in the previous reports. Make sure to mark these references, as well as the ones pointing to experimental results clearly. \textit{Missing parts of the above requirements might lead to significant loss of points in each section.}

The report should be organized in sections as explained in the next pages, and each section should address at least the questions mentioned for each point. You might be called for a meeting in person to clarify aspects of the report or the system and to make a short presentation of the work done. By submitting the report, you  confirm  that  you  have  done  the  work  on  your  own,  the  data used comes  from  experiments  your have  done,  you  have  written  the  report  on  your  own,  and  you have  not  copied  neither text nor data from other sources.

\medskip
The milestone is worth 200 points. 


\pagebreak


\section{System as One Unit}\label{sec:system-one-unit}

\iffalse
Length: 1-2 pages

Build an M/M/1 model of your entire system based on the stability trace that you had to run for the first milestone. Explain the characteristics and behavior of the model built, and compare it with the experimental data (collected both outside and inside the middleware). Analyze the modeled and real-life behavior of the system (explain the similarities, the differences, and map them to aspects of the design or the experiments). Make sure to follow the model-related guidelines described in the Notes!
\fi


In M/M/1 model we have one queue and one server. As server we will consider memcached server. Since there are actually 3 memcached servers processing requests, we will have to calculate the service rate accordingly. We do not consider clients as a part of our system - as the system we take our middleware and connected memcached servers.

For this model we take server discipline First Come First Served, with no buffer limitations. The population size is 192 clients and the system is a closed one.

Throughput measured by the memaslap clients was equal to 13758 operations/second. Therefore, I take arrival rate $\lambda = 13758 \textrm{ ops/s}$.

One approach to calculate the service rate is to take maximum throughput measured during the experiment. This approach has been suggested during the tutorial and gives us a lower bound on a service rate. This is because if system was able at some point in time to have this performance, it means that it is able to perform at least that well. Since the running time of the experiment was pretty long (30 minutes), we can assume that this approximation is good enough.

Another approach to do this would be to calculate the number of requests being actively processed in the middleware, which corresponds to the average parallelism in the middleware. We would then assume that the service time is average time spent in the servers divided by the number of active threads. We still would need to take into the account that this time includes requests being sent over the network, but we could also treat network as a queue. Since servers have only 1 thread working the requests will partially wait in the queue of the server and partially this waiting period will be included in the time spent in the network. However, this approach proved to be inaccurate. For this task I got the following results: on average 38.15 getter threads are busy processing requests and there are on average 1.05 requests in all setter threads (there are 3 of them). So, on average, there are $m = 39.21$ on the fly (sum based on more precise numbers). That would mean service rate approximately 13893 requests/second and would result in a very high utilization - around 99\%. For the second task, however, utilization calculated using this approach resulted in utilization over 1. This is because in order to use this method we have to assume that memcached servers are busy almost all the time and time spent in the network, even without network, would be spent in the queue. However, this is not the case and therefore results of this method are unsatisfactory. 

%I had to improve the logging in my middleware (see apendix) in order to log also current numbers of busy getter threads and number of requests processed currently by setter requests (i.e. number of set requests that have been taken out of the queue but the response for them has not yet been sent to the client). This made it necessary to repeat the experiment.

%Looking at the middleware logs I have calculated, that an average time spent in the servers equals: $T_s = \SI{2841.63}{\micro\second}$, which is the difference between $T\_SendToClient$ and $T\_Dequeue$ (see report for milestone 1.). When it comes to number of requests on the fly I got the following results: on average 38.15 getter threads are busy processing requests and there are on average 1.05 requests in all setter threads (there are 3 of them). So, on average, there are $m = 39.21$ on the fly (sum based on more precise numbers). Based on the previous analysis we can now calculate the service rate appropriate for M/M/1 model:

Looking at the memaslap logs I have calculated that maximum throughput was equal to 16701 requests/second. Therefore, using the first described approach, we have:
$\mu = 16701\textrm{ req/s}$.
%$$\mu \approx \frac{39.21}{2841.63} \cdot 10^6 \textrm{ requests/second} \approx 13892.53 \textrm{ requests/second}$$
%In order to calculate mean service rate which includes that many requests are processed in parallel, I had to find out the average number of requests being in the service. In order to do that I had to make some changes in my code (described in the appendix) i I 
This enables us to calculate the offered load:
$\rho \approx 82.38\%$
The offered load is below 1, which means that the system is stable.

The utilization can be considered to be pretty high. It stems from the fact that for stability experiment I have used only 16 threads. As the experiments from second milestones have shown, for 16 threads we get worse performance than for the higher number of threads. This means that threads are mostly busy and since the high utilization of the system. What is more, the component that causes this high utilization is accepting thread. As I have pointed out in the report for milestone 2, this component at some point becomes the bottleneck of the system. For stability trace It was still not the case (too low number of threads), but this component either way is not that effective and affects the offered load.

% Maybe remove
It is also worth pointing out, that the average number of set requests being serviced is quite low, considering that there are 3 setter threads (one for each queue) which process requests asynchronously (one thread can process more than one requests at the same time). This is because set requests are only 1\% of all requests, so they are not many of them and although it takes longer to process them, the low number of them leads to low number of them being processed at the same time.

The  probability that there are zero jobs in the system equals: $1 - \rho \approx 17.62\%$. 
This result is not consistent with the data collected from the middleware. We can calculate that the first percentile of number of active getter threads is already equal 8. This means that 99\% of the time they are not less than 8 get requests being processed in the system. This contradicts result from the theory. This can be explained in a following way: M/M/1 models do not take into the account the possibility that several requests can be processed at the same time. Based on the high service rate from model we can get that the single request is processed really quickly, so that they don't queue that much. However, in reality, requests are processed for a longer time, but they are several requests processed in parallel. In a similar way, when we calculate the average number of requests in the system, we get contradicting results. We have:
$E[n] 
%=\frac{\rho}{1 - \rho} \approx{82.38\%}/{17.62\%} 
\approx 4.67$.
We know from the collected logs that on average there are around 39 requests being processed at the same time. This means that they are at least 39 requests in the whole system. The reason for this discrepancy is the same as mentioned above - M/M/1 model does not consider the parallelism. 

When we take a look at calculated mean number of jobs in the queue and in the service we still see discrepancies, but there is one thing that is similar to measured values:
$E[n_q] 
%= \frac{\rho^2}{1-\rho} 
\approx 3.85$ and
$E[n_s] 
%= \rho 
\approx 0.82$.
The ratio of jobs in the queue to jobs in the system is equal to $\rho \approx 0.823$ (simple formulas transformation). Under the assumption that almost all of 192 requests are in the system, we have that around 39 requests are being processed (see earlier) and therefore $192-39 = 153$ jobs are in the queue. The ratio between jobs in the queue and jobs in the system equals here to $\frac{153}{192} \approx 0.797$. These values are relatively close to each other, so we can say that this ration has been well predicted by our model.

%The probability that there are $n$ jobs in the system equals approximately $p_n = 0.97\% \cdot 99.03\%^n$. We can also calculate how many requests are in system in total:
%$$E[n] = \frac{\rho}{1 - \rho} \approx{99.03\%}/{0.98\%} \approx 102.52$$
%Let me remind here that there are 192 clients. Calculated number is therefore plausible. Requests spend also significant amount of time traveling in the network between memaslap clients and middleware, which causes the calculated number of requests in the system to be a lot less than the number of clients.

We can calculate also mean response time:
$E[r] 
%= (1/\mu)/(1 - \rho) \approx \frac{1}{16701 \textrm{ requests/second} \cdot 17.62\%} 
\approx \SI{339.82}{\micro\second}$
When we look at the data collected from the middleware we can see that the mean time spent in the middleware equals approximately \SI{7655}{\micro\second}. This does not correspond at all to the calculated mean response time. This can again be explained by the fact that our model is not appropriate for handling parallelism. High service rate means, for M/M/1 model, that the requests are processed quickly and do not spend much time in service and, in consequence, in the middleware. In reality it takes some time to process the request, which results in way higher measured time spent in the middleware. The same applies for mean waiting time (time in the queue) and mean service time. From theory we have:
$E[w] 
%= \rho \frac{1/\mu}{1 - \rho} \approx 82.38\% \cdot \frac{1/16701}{17.62\%} 
\approx \SI{279.95}{\micro\second}$ and
$E[s] 
%= \frac{1}{\mu} 
\approx \SI{59.88}{\micro\second}$.
The corresponding values measured by the middleware equal: \SI{3203}{\micro\second} for time spent in the queue (which is part of the system representation) and \SI{2841}{\micro\second} for the time spend in the servers. We can see that measured time spent in the servers is approximately 47 times higher then calculated service time. This value is comparable to 39 requests on the fly (mentioned earlier), but it better corresponds to the parallelism of the middleware, since it also reflects the fact that servers do not have to be busy all the time. Let's not forget here that requests also spend some time waiting for the accepting thread to be parsed, so that the difference between waiting time predicted and measured is even higher.

%TODO: more statistics
To sum up, M/M/1 model does not reflect our middleware properly. It does not take into the account the parallelism and predicted values differ significantly. I have not included calculations for more values, like variance or percentiles, cause they also won't correspond to the values measured by the middleware.

% TODO: explain
%$$E[r] = \frac{1/\mu^2}{(1 - \rho)^2} \approx \frac{1/13893^2}{0.97\%^2}$$
%The 90th percentile calculated equals to : $2.3 E[r] \approx \SI{17138}{\micro\second}$. When we look at the data from middleware we can see that 90th percentile actually equals around \SI{13880}{\micro\second}, which is lower than the time calculated from the model.
% TODO: explain

%We can also calculate the time spent in the queue:
%$$E[w] = \rho \frac{1/\mu}{1 - \rho} \approx 99.03\% \cdot \frac{1/13893}{0.97\%} \approx \SI{7379}{\micro\second}$$
%Mean time spent in the queue (being part of the implementation) measured by the middleware is around \SI{4768}{\micro\second}. The differences here are much more significant then overall response time. This is because the queue from the implementation does not correspond to the queue in the M/M/1 model. Requests have to wait also in the queue for each server and also spend some time in the network (which can be treated as a part of the queue, as explained above). What is more, since we have 3 servers, more requests than 1 can be processed by them in parallel, which means that the time spent in the queue measured will be even lower than this calculated.

%When we look at the data collected from the middleware we can see that the mean time spent in the queue (being part of the implementations) is equal to around \SI{3203}{\micro\second}. The difference between this values stems from the fact that in our model we have one queue, which includes also waiting by the request to be handled by the thread handling network messages. As described in the report for milestone 2., this point of the system becomes at some point the bottleneck, so it's reasonable that waiting time here is also significant.

\clearpage

\section{Analysis of System Based on Scalability Data}\label{sec:analysis-scalability}

\iffalse
Length: 1-4 pages

Starting from the different configurations that you used in the second milestone, build M/M/m queuing models of the system as a whole. Detail the characteristics of these series of models and compare them with experimental data. The goal is the analysis of the model and the real scalability of the system (explain the similarities, the differences, and map them to aspects of the design or the experiments). Make sure to follow the model-related guidelines described in the Notes!
\fi

In this section we will model a system as a queue with $m$ servers processing the requests. The system under test is going to be defined as our middleware together with memcached servers. We will take $m$ equal to the number of memcached servers used for the given experiment and in our model each server will correspond to threads associated with one queue forwarding request together with the according memcached server. As it will be pointed out, this model won't correspond properly to the behavior of the system, since each server can in fact process more than 1 request at the same time (which is the underlying assumption for the model). I have decided for this model because the number of resources (servers) here corresponds to the actual number of core functionality resources (memcached servers; middleware is only for forwarding requests, not actually providing core functionality of the whole system). Since I include the processing threads as a part of the service, I don't have to change the arrival rate to include increased number of requests sent to the memcached servers because of the replication. Actual number of requests that arrive to the threads is the same as number of requests sent to the clients and replication will be included in the service time.

For this model we take server discipline First Come First Served, with no buffer limitations. The population size is 210 clients and the system is a closed one.

To create models for this section I have used the approach presented in the section \ref{sec:system-one-unit}, which is calculating service rate based on the maximum throughput. In the experiments described in this section runtime was shorter than in the stability experiment, but still we collected 180 values for the throughput (measuring every second for one minute, 3 replications). This enabled us to, once again, achieve quite good approximation. Another approach (dividing server time by the number of requests on the fly) would result in utilization over 1.

In order to provide a proper analysis of the model, I had rerun the experiments from milestone 2. tasks 2. and 3. I have used the same configuration as provided in sections 2.2 and 3.2 of my report from the previous milestone. The only difference was in the logging facilities introduced for the middleware (for changes in the source code - see appendix). The new log files have corresponding names to those used in the previous milestones, but have 'improved\_' prefix (for log files list - see appendix).

\iffalse
In the scalability experiments analysis we also have to account for the fact that having replication increases load on the servers. Let us then introduce following symbols for parameters and measured values: $T$ - measured mean throughput, $M$ - measured maximum throughput, $w$ - writes percentage, $r$ - numbers of servers to which we replicate, $m$ - number of servers. Lets also take $N$ as the number of requests send by the clients to the middleware. Distinguishing between get and set requests, we have that all servers have to process $N'$ requests, where:
$$N' = N (1 - w) + wrN = N (1 + w(r - 1))$$
When we aggregate number of requests during one second and take an average, we can calculate the mean arrival rate:
$$\lambda = T (1 + w(r - 1))$$
When we want to calculate now the service rate based on the maximum throughput, we also need to include this factor. This is because we're actually counting the number of requests being processed by the middleware at the peak performance, so if we modify the number of requests that arrive to the system, we also need to modify the number of requests that get out of the system accordingly. Therefore, when we include that there are $m$ servers processing requests from one queue, we have:
$$\mu = \frac{M  (1 + w(r - 1))}{m}$$

When we now calculate the traffic intensity, introduced additional factors which account for replication, are reduced:
$$\rho = \frac{\lambda}{m \mu} = \frac{T}{M}$$
\fi

There have been many different configurations used in the scalability experiments from previous milestone (task 2. and 3.). In order to provide a detailed analysis of created models I have focused my attention on creating models for extreme configurations - 3 and 7 servers, no replication and full replication, 1\% and 10\% of write requests. This will result in creating 8 models. We want to analyze the scalability of our system so I will look into increasing number of servers under 4 configurations (possible configurations for extreme values of replication factor and writes percentage; effect of these factors on performance will be analyzed in section \ref{sec:2k-experiment}).

\subsection{Input data for models}

In the table \ref{fig:input} is presented the necessary data to create before mentioned models. I have calculated $\mu$ based on above formula.

\begin{figure}
\label{fig:input}
\begin{center}
\begin{tabular}{|$c|^c|^c|^c|^c|^c|^c|^c|}
\hline
\rowstyle{\bfseries} 
$m$	& Rep.	&	 \parbox[t]{2cm}{Writes percentage (\%)} &	T	&	$\lambda$	&	M	&	$\mu$	 \\

\hline 3 & none & 1 & 15687 & 15687 & 19860 & 6620 \\ 
\hline 7 & none & 1 & 12730 & 12730 & 16187 & 2312 \\ 
\hline 3 & all & 1 & 15556 & 15556 & 19590 & 6530 \\ 
\hline 7 & all & 1 & 12070 & 12070 & 14769 & 2110 \\ 
\hline 3 & none & 10 & 15340 & 15340 & 19227 & 6409 \\ 
\hline 7 & none & 10 & 12547 & 12547 & 15954 & 2279 \\ 
\hline 3 & all & 10 & 14353 & 14353 & 17807 & 5936 \\ 
\hline 7 & all & 10 & 10139 & 10139 & 13592 & 1942 \\ 
\hline
\end{tabular}
\caption{Input data}
\end{center}
\end{figure}

Let us point out here that $\mu$ (for each server) is significantly decreasing as we increase the number of servers. This should not be the case according to the theory - $\mu$ should be the same for each server, independently on their number and increasing number of servers should only increase the effective service rate ($m\mu$). However, in theory increasing number of servers means increasing number of resources available. We increase the number of resources available for memcached servers, but the number of resources available on the side of middleware stay the same (we have only 8 cores), what is more, with more servers we have more threads, which even more decreases the performance and results in decreased $\mu$.

\subsection{Number of requests}

Based on this data we can compute system properties. I have used formulas from Box 31.2 in the course book.

Let us define: $P_0$ - probability that there are 0 jobs in the system, $P_q$ - probability of queuing, $R_g$, $R_s$, $R$ - number of, respectively, get, set and all requests on the fly. Rep. is an abbreviation for replication factor and w\% - for percentage of write requests. Other definitions are the same as in the book.

{\small
\begin{center}
\begin{tabular}{|$c|^c|^c|^c|^c|^c|^c|^c|^c|^c|^c|^c|^c|}
\hline
\rowstyle{\bfseries} 
$m$	& Rep.	&	W\% & $\rho$ & $P_0$ &	$P_q$	&	$E[n]$ 	&	$E[n_q]$	& $E[n_s]$ &	$R_g$ & $R_s$ & $R$ & $\frac{R}{E[n_s]}$  \\
\hline 3 & none & 1 & 0.790 & 0.060 & 0.631 & 4.74 & 2.37 & 2.37 & 52.06 & 1.21 & 53.27 & 22.48 \\ 
\hline 7 & none & 1 & 0.786 & 0.003 & 0.458 & 7.19 & 1.69 & 5.51 & 72.87 & 1.17 & 74.03 & 13.45 \\ 
\hline 3 & all & 1 & 0.794 & 0.058 & 0.638 & 4.84 & 2.46 & 2.38 & 54.66 & 1.25 & 55.91 & 23.47 \\ 
\hline 7 & all & 1 & 0.817 & 0.002 & 0.523 & 8.06 & 2.34 & 5.72 & 71.91 & 1.26 & 73.17 & 12.79 \\ 
\hline 3 & none & 10 & 0.798 & 0.057 & 0.644 & 4.93 & 2.54 & 2.39 & 48.89 & 12.48 & 61.37 & 25.64 \\ 
\hline 7 & none & 10 & 0.786 & 0.003 & 0.458 & 7.19 & 1.69 & 5.50 & 65.90 & 11.91 & 77.82 & 14.14 \\ 
\hline 3 & all & 10 & 0.806 & 0.054 & 0.657 & 5.15 & 2.73 & 2.42 & 48.02 & 13.23 & 61.25 & 25.33 \\ 
\hline 7 & all & 10 & 0.746 & 0.005 & 0.380 & 6.34 & 1.11 & 5.22 & 67.63 & 12.78 & 80.41 & 15.40 \\ 

\hline
\end{tabular}
\end{center}
}

From above table we can see that traffic intensity (utilization) for each server does not differ much as we increase number of servers and vary the configuration. The most significant difference can be seen for configuration (all, 10) (first element - describes replication factor, second - writes percentage). There increasing number of servers result in decrease in traffic intensity of around 0.06. In theory, 
%traffic intensity should decrease as we increase number of servers - we should be able to achieve higher service rate, since service rate for each server should be independent and increasing their number should increase the effective service rate. 
with more servers load for each servers should also decrease, so it should result in the decrease in the traffic density. However, the bottleneck of the whole system is middleware, not memcached servers, which is not well illustrated by our model. This means that as we increase number of servers load on the middleware is not decreasing and therefore traffic intensity does not change that much.

It must be pointed out here that average utilization of each server is equal to traffic intensity. 

As we increase the number of servers, $P_0$ value significantly decreases. This is understandable, since for 7 servers we get worse performance (as analyzed in the report for milestone 2.), so most of the time some requests will be in the middleware. 
% TODO: add 1st percentile for number of requests
The probability of queueing is also decreasing as we increase the number of servers. This corresponds to our system design - with more servers we have more threads which take requests from the queue. Although altogether they might not be processed faster, workers can have take them out of the queue more effectively. Even when the worker after worker takes out the request from the queue looses it's processing time (becomes idle), the request is already being considered as being processed. %TODO: does this make sense

From above table we can also see that the calculated number of requests being service does not correspond at all to the number of requests on the fly. The actual number of requests on the fly is always at least 13 times higher than the calculated value. 
This is because our model does not take into the account the parallelization gained from dozens of threads sending the requests to the memcached servers. In M/M/m model we can have only maximally 3 jobs being processed at the same time. However, our middleware was able to process 90 get requests (for 3 servers) and 210 get requests (for 7 servers) in parallel, as well as theoretically unbounded number of set requests. This is why these values differ so much. When we calculate the ratio of number of measured requests on the fly and calculated number of requests being service we can clearly see that it is decreasing around 1.6 - 1.9 times. This is because we have only limited number of cores available for our middleware. When increasing number of servers we also increase the number of processing threads. Since there are only 8 cores, more threads mean less CPU time for each thread. The threads cannot perform there that well and less of them are used (some of them may not even at all be granted some CPU time). Therefore with increase of servers we notice less increase in the measured number of requests on the fly than for the calculated value (which does not represent well limited number of cores). 

%With introducing middleware we add overhead to processing the request and increase the response time (comparing to the case when we send requests directly to the memcached servers). 
 
I have not collected data for number of requests in the queue. It would also significantly differ from above calculations and one could expect that the ratio would be similar to the ratio already calculated.

%As we increase number of servers we can clearly see the increase in the number of get requests in the server. This corresponds well to the design of our system - increasing number of servers means increase in the number of getter threads processing requests. So more requests can be processed at the same time. We can also actually see the increase in the number 

It is worth noticing here that similar problems with huge differences between calculated number of requests and measured is similar to the case with M/M/1 model. Here we account for a little more parallelization than in the previous section, but we still don't consider many requests being processed in parallel for one server inside the middleware. One could try to take as an a number of servers the total number of threads in thread pools or the number of requests in the fly. However, this would not correspond to processing of the set requests - they are processed in the different way inside our middleware. Therefore that model might also not fit properly our system. Only network of queues described in the section \ref{sec:network-of-queues} could account for the whole parallelism available in our system (but it will still not correspond well to the real-word situation with limited number of cores).

% TODO: add variance

%TODO: have I mentioned decrease in ratio of active threads???

\subsection{Response times and times breakdown}

Let us define: 
%$T$ - mean response time measured by the memaslap clients, 
$T_r$ - time spent in the middleware, $T_w$ - time spent in the queue, $T_s$ - time spent in the servers. 

%TODO: maybe add response time from clients ...
% TODO: make it fit
{\small
\begin{center}
\begin{tabular}{|$c|^c|^c|^c|^c|^c|^c|^c|^c|^c|^c|^c|^c|^c|}
\hline
\rowstyle{\bfseries} 
$m$	& Rep. & W.  & $E[r]$ & $E[w]$ & $E[s]$ & $T_r$ & $T_w$ & $T_s$ & $\frac{T_w}{E[w]}$ & $\frac{T_s}{E[s]}$  & $\frac{R}{m}$ & $\frac{E[s]}{E[w]}$ & $\frac{T_s}{T_w}$  \\
\hline 3 & none & 1 & 302 & 151 & 151 & 6005 & 2551 & 3419 & 16.88 & 22.63 & 17.76 & 1.00 & 1.34 \\ 
\hline 7 & none & 1 & 565 & 132 & 432 & 8818 & 3056 & 5698 & 23.07 & 13.18 & 10.58 & 3.26 & 1.86 \\ 
\hline 3 & all & 1 & 311 & 158 & 153 & 6342 & 2706 & 3592 & 17.13 & 23.46 & 18.64 & 0.97 & 1.33 \\ 
\hline 7 & all & 1 & 668 & 194 & 474 & 9370 & 3226 & 6089 & 16.66 & 12.85 & 10.45 & 2.45 & 1.89 \\ 
\hline 3 & none & 10 & 322 & 166 & 156 & 6770 & 2511 & 4217 & 15.16 & 27.03 & 20.46 & 0.94 & 1.68 \\ 
\hline 7 & none & 10 & 573 & 134 & 439 & 9342 & 3124 & 6135 & 23.25 & 13.98 & 11.12 & 3.26 & 1.96 \\ 
\hline 3 & all & 10 & 359 & 190 & 168 & 6836 & 2400 & 4389 & 12.62 & 26.05 & 20.42 & 0.89 & 1.83 \\ 
\hline 7 & all & 10 & 625 & 110 & 515 & 11554 & 3526 & 7951 & 32.07 & 15.44 & 11.49 & 4.68 & 2.25 \\ 
\hline
\end{tabular}
\end{center}
}

I will compare times from the solution of the model to times measured by the middleware. I am not focusing my attention on response time from memaslap logs, since clients are not part of my system (according to my definition) and the actual response time of the system should correspond to the time spent in the middleware.

When we compare measured times with times calculated by the model we can, once again, see huge discrepancies. Times calculated by the model are many times lower than these measured. This can again be explained by not taking the whole parallelization into the account when constructing the model. In order to account to the same effective service rate as measured, our 3 and 7 servers have to process the requests very quickly. In the real system processing the requests does not have to be very fast, because we can process many requests in parallel and each of this processing can take longer time. As we decrease the parallelism (when going from our implementation to the model), in order to keep the same service rate, response time has to decrease accordingly.

The ration between corresponding times are, however, more or less similar to the number of requests on the fly per each memcached server. This is because $\frac{R}{m}$ factor is the parallelism factor that our model is missing. We can clearly see that ratio of service time and servers time is always greater than corresponding number of active threads. This corresponds to the results I already mention in the section \ref{sec:system-one-unit}. This can be explained by the fact that memcached servers are not the bottleneck of the system. If ratio of service to servers time would be similar to number of requests on the fly, it would mean that memcached servers are almost all the time busy and time spent in the network would otherwise be spent in the queue of the memcached server. This is not the case, so servers time measured must be higher than $E[s]\frac{R}{m}$ - some of this time is also spent in the network while memcached servers are idle and waiting for new requests.

We can also see clearly see that ratio of measured and calculated waiting time is smaller than ratio of measured and calculated service (servers) time for $m=3$ and greater for $m=7$. This is because as we increase the number of servers, the ratio for service time is decreasing, also like number of requests on the fly per server. Number of requests on the fly per server decreases, because with more threads we have more operating system and context switching overhead and our threads cannot be that effective and therefore lower number of them is user. On the other hand, ratio for waiting time is almost always increasing (when we go from 3 to 7 servers; apart from (all, 1) configuration). This is because with more servers waiting time predicted by the model usually increases (since we have lower number of requests per server), but measured time spent in the queue always increases (because of more overhead connected with increase in the number of threads).

As we increase number of servers, the ration of service time to waiting time increases, both for calculated and measured values. This is because time spent in the servers is increasing, but calculated waiting time is usually decreases and measured waiting time increases slower (the increase in the waiting time is connected with the overhead on the middleware, increase in the waiting time is also connected with establishing more network connections and overhead of the network).

%As we increase replication factor and writes percentage (together or separately) the ratio between expected service time and expected waiting time is usually decreasing (apart from one case, when it doesn't change significantly). 

\subsection{Conclusion}
As I have shown in my analysis, M/M/m model do not correspond well to the implemented system and do not account for the whole parallelization of the system. As we scale our system (increase the number of servers), the model starts to better approximate the real behavior of the system. This is because we increase the parallelism of the model, we also increase the parallelism for our implementation, but the limitation in the number of cores prevent us to increase this parallelism as much as we would like to (if we wanted to maximize the performance of our middleware).

It is also worth pointing out here that we have created models that combine set and get. Both of these type of requests have different statistics and are processed in the different way. However, such simple model as M/M/m would be unable to include these different ways of processing requests. A proper distinction between these types will be introduced in the next section.

\clearpage

\section{System as Network of Queues}\label{sec:network-of-queues}

Length: 1-3 pages

Based on the outcome of the different modeling efforts from the previous sections, build a comprehensive network of queues model for the whole system. Compare it with experimental data and use the methods discussed in the lecture and the book to provide an in-depth analysis of the behavior. This includes the identification and analysis of bottlenecks in your system. Make sure to follow the model-related guidelines described in the Notes!



\clearpage

\section{Factorial Experiment}\label{sec:2k-experiment}

\iffalse
Length: 1-3 pages

Design a $2^k$ factorial experiment and follow the best practices outlined in the book and in the lecture to analyze the results. You are free to choose the parameters for the experiment and in case you have already collected data in the second milestone that can be used as source for this experiment, you can reuse it. Otherwise, in case you need to run new experiments anyway, we recommend exploring the impact of request size on the middleware together with an other parameter.
\fi

In the milestone 2 we have analyzed thoroughly the impact of the replication factor and impact of the writes percentage on performance of the system. We have not, however, concentrated our attention on which of these factors plays the biggest role when it comes to the performance of the system. Therefore, for this factorial experiment I have decided to investigate the impact of two factors: replication factor and percentage of writes requests. This enabled me to reuse data from the milestone 2, task "Effect of Writes" (using logs {\it writes\_general} from the previous report). We will look at the behavior of 5 servers (since for this number of servers we have chosen optimal number of threads - see report for milestone 2, task "Maximum Throughput").

We will present here $2^kr$ factorial design experiment with repetitions, where $k = 2$ (2 parameters) and $r = 3$ (3 repetitions for each configuration).

Let $A$ be the percentage of writes parameter and $B$ be the replication factor parameter. We will look at the extreme values for these two parameters - 1\% and 10\% as writes percentage; none and all as replication factor. Let us now define two variable $x_A$ and $x_B$ as follows:
\begin{gather*}
x_A = 
\begin{cases}
-1 \qquad \text{ if writes percentage equals 1\%} \\ 
\;\;\;1 \qquad \text{ if writes percentage equals 10\%} \\ 
\end{cases}
\end{gather*}

\begin{gather*}
x_B = 
\begin{cases}
-1 \qquad \text{ if replication factor is none} \\ 
\;\;\;1 \qquad \text{ if replication factor is all} \\ 
\end{cases}
\end{gather*}

As the metric for measuring performance we choose aggregated throughput. Selecting additional metric - response time - would be redundant since in close system according to interactive response time law values to these 2 parameters are strictly corresponding.

We can now regress on $x_A$ and $x_B$ the performance of the system using a nonlinear regression model of the form:
$$y = q_0 + q_Ax_A + q_Bx_B + q_{AB}x_Ax_B$$
In order to calculate the coefficients, we will use sign table presented in the book.
\medskip

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline $I$ & $A$ & $B$ & $AB$ & $y$ &Mean $\bar{y}$ \\
\hline	1	&	-1	&	-1	&	1	&	(	12833	,	12684	,	12974	) &	12830	\\
\hline	1	&	1	&	-1	&	-1	&	(	12732	,	12038	,	12121	) &	12297	\\
\hline	1	&	-1	&	1	&	-1	&	(	12140	,	12667	,	12533	) &	12447	\\
\hline	1	&	1	&	1	&	1	&	(	10582	,	11077	,	10188	) &	10616	\\
\hline	48190	&	-2364	&	-2065	&	-1298	&							&	Total	\\
\hline	12047	&	-591	&	-516	&	-324	&							&	Total / 4	\\
\hline
\end{tabular}
\end{center}
\medskip

From the table above we now see calculated effects ($q$ coefficients): $q_0 \approx 12047$, $q_A \approx -591$, $q_B \approx -516$, $q_{AB} \approx -324$.

We can now use the model to estimate the experimental errors. Using the effects calculated above we have:
$$\hat{y_i} =  q_0 + q_Ax_{Ai} + q_Bx_{Bi} + q_{AB}x_{Ai}x_{Bi},$$
where $\hat{y_i}$ is the estimated performance when factors $A$ and $B$ are at levels $x_{Ai}$ and $x_{Bi}$ respectively.

The error for $j$th replication of $i$th experiment is given by the formula:
$$e_{ij} = y_{ij} - \hat{y_i} = y_{ij} -  (q_0 + q_Ax_{Ai} + q_Bx_{Bi} + q_{AB}x_{Ai}x_{Bi})$$

We can calculate the error looking at the data collected from the experiment. It is worth noticing here, that $\hat{y_i}$ is equal to the mean of values from $i$th experiment. The results are presented in the following table:
\medskip

\begin{center}
\begin{tabular}{@{}|c|cccc|c|ccc|ccc|}
\hline & \multicolumn{4}{c|}{Effect} & \parbox[t]{2.1cm}{Estimated performance} & \multicolumn{3}{c|}{Measured performance} & \multicolumn{3}{c|}{Errors} \\
\hline & $I$ & $A$ & $B$ & $AB$ & & & & & & & \\
\hline i & 12047	&	-591	&	-516	&	-324 & $\hat{y_i} $ & $y_{i1}$ & $y_{i2}$ & $y_{i3}$	& $e_{i1}$ & $e_{i2}$ & $e_{i3}$ \\
\hline	1	&	1	&	-1	&	-1	&	1	&	12830	&	12833	&	12684	&	12974	&	3	&	-146	&	144	\\
\hline	2	&	1	&	1	&	-1	&	-1	&	12297	&	12732	&	12038	&	12121	&	435	&	-259	&	-176	\\
\hline	3	&	1	&	-1	&	1	&	-1	&	12447	&	12140	&	12667	&	12533	&	-307	&	220	&	86	\\
\hline	4	&	1	&	1	&	1	&	1	&	10616	&	10582	&	11077	&	10188	&	-34	&	461	&	-428	\\
\hline
\end{tabular}
\end{center}
\medskip

We can now calculate the sum of the squared errors - $SSE$ - which we will use later to estimate the variance of the errors:
$$SSE = \sum_{i = 1}^{2^2} \sum_{j = 1}^3 e_{ij}^2 = 876248$$

Now we want to calculate the allocation of variation. The total variation or Total Sum of Squares is equal to:
$$SST = \sum_{i, j} (y_{ij} - \bar{y})^2$$
and can be divided into four parts (which is proved in the book):
$$ \sum_{i, j} (y_{ij} - \bar{y})^2 = 2^2\cdot 3 \cdot q_A^2 + 2^2\cdot 3 \cdot q_B^2 + 2^2\cdot 3 \cdot q_{AB}^2 + \sum_{i, j} e_{ij}^2$$
$$SST = SSA + SSB + SSAB + SSE,$$
where $SS*$ corresponds to the appropriate part from the formula above. Let us now calculate these values based on experimental results (values rounded to the nearest integer):
$$SS0 = 2^2\cdot 3 \cdot q_0^2 = 1741682980 $$
$$SSA = 2^2\cdot 3 \cdot q_A^2 = 4192554$$
$$SSB = 2^2\cdot 3 \cdot q_B^2 = 3198169$$
$$SSAB = 2^2\cdot 3 \cdot q_{AB}^2 = 1262954$$

Now we can calculate $SST$:
$$SST = SSA + SSB + SSAB + SSE = 9529925$$
That enables us to divide the total variation into to parts. When we mark variation as $V$ variable with appropriate prefix, we have:
$$VA =  4192554/ 9529925 \approx 43.99\%$$
$$VB =  3198169 / 9529925 \approx 33.56\%$$
$$VAB =  1262954 / 9529925 \approx 13.25\%$$
$$VE =  876248 / 9529925 \approx 9.19\%$$
We can therefore conclude that facotr $A$ - percentage of writes - explains around 44\% of variation and is the most significant factor. Factor $B$ - replication factor - explains  around $34\%$ of the variation, interaction $AB$ - around 13\%. The remaining around 9\% is unexplained and is attributed to errors.
%TODO analyze in context of the system

Both of the parameters analyzed in this experiment cause the decrease in the performance as we increase them. Both these parameters increase the number of set requests that have to be sent to the memcached servers. However, increasing the writes percentage also decreases the reads percentage. Above analysis has shown that most impact can be attributed to increasing the writes percentage rather than increasing the replication factor, although the difference is not that big.

These results can be explained by a couple of factors. Firstly, by increasing the number of writes from 1\% to 10\% we increase the number of set requests by 10 times. Increasing replication factor from none to all for 5 servers means increasing the number of set requests sent to memcached servers 5 times. For $A$ factor we then increase the number of requests twice as much as for the $B$ factor, which can lead to more significant decrease in performance.

Secondly, in the ideal system increasing replication factor should have no or very little impact on the performance of the middleware (as explained in the report for milestone 2). On the other hand, increasing the number of setter requests can, also theoretically, have a significant impact on the performance of the system. This is because for each server we have only one thread processing set requests. When we increase the number of threads by 10 times, this thread will have to serve 10 times more requests and more requests will simply have to wait in the queue, which will decrease the performance of the system. Although in a real world system replication facto has an effect on the system performance, it should be less significant than increasing the writes percentage. 

Let us also analyze impact of the interaction between factors. It is not as significant as one could expect. In the milestone 2 we have shown that the decrease in the performance as we increase the writes percentage becomes significant with replication factor all, whereas for replication factor none it is not so significant. But also similarly we can see that it is only for 10\% writes that we can notice significant decrease in performance as we go from no replication to replication to all. Conducted calculations enable us to say then that it's mostly the influence of each factor separately that causes the significant decrease for the worst-case scenario (10\% writes with replication to all servers). This might be explained by the fact that these factors cause decrease in performance differently. Increasing replication makes us more prone to network latencies and therefore server time for requests is higher which leads to higher response time. On the other hand, increasing writes percentage causes setter worker to have more work to do and increases the ration of slower-processed requests. These ways of decreasing the performance are not that dependent on each other, which is confirmed by the factorial analysis conducted above.

\pagebreak

\section{Interactive Law Verification}\label{sec:interactive-law}

\iffalse
Length: 1-2 pages

Check the validity of all experiments from one of the three sections in your Milestone 2 report using the interactive law (choose a section in which your system has at least 9 different configurations). Analyze the results and explain them in detail.
\fi

I will check the validity of my experiments from section 2. of the Milestone 2 report, where I was analyzing the behavior as we change replication and the number of servers. The number of clients for all configurations was equal to 210. We assume that time clients wait before submitting next request equals approximately to 0 - it is reasonable assumption since every client corresponds to one thread and processing responses and issuing requests 

Below table presents the results of the experiments for different configurations as well as calculated response time. It has been calculated from the formula: $R = \frac{N}{X} - Z$, where $R$ - response time, $N = 210$ - number of clients, $X$ - throughput and $Z=\SI{0}{\micro\second}$ - waiting time for clients before submitting next request. I have also calculated derivation from the theory, which is equal to $\frac{R_e - R}{R}$, where $R_e$ is measured response time.
\medskip

\begin{tabular}{|c|c|c|c|c|c|}
\hline \parbox[t]{2.2cm}{\bf{Replication\\factor}} & \parbox[t]{1.8cm}{\bf{Number of \\servers}} & \parbox[t]{1.5cm}{\bf{TPS \\ \lbrack ops/s \rbrack}} & \parbox[t]{2cm}{\bf{Resp. time [us]\\(measured)}} & \parbox[t]{2.2cm}{\bf{Resp. time [us] \\(interactive law)}} & \parbox[t]{2.4cm}{\bf{Deviation \\ from theory}} \\[3ex]
\hline	none	&	3	&	11884	&	17684	&	17670	&	0.08\%	\\
\hline	half	&	3	&	11969	&	17590	&	17545	&	0.26\%	\\
\hline	all	&	3	&	11641	&	18108	&	18039	&	0.38\%	\\
\hline	none	&	5	&	12169	&	17273	&	17257	&	0.09\%	\\
\hline	half	&	5	&	12046	&	17496	&	17433	&	0.36\%	\\
\hline	all	&	5	&	11231	&	18773	&	18698	&	0.40\%	\\
\hline	none	&	7	&	11817	&	17845	&	17771	&	0.42\%	\\
\hline	half	&	7	&	11048	&	19057	&	19008	&	0.26\%	\\
\hline	all	&	7	&	10145	&	20732	&	20701	&	0.15\%	\\
\hline
\end{tabular}
\medskip

As we can see from above table, the interactive response time law corresponds well to the results from the measurements. Looking at the deviation we can see that response time measured has always been higher than response time calculated using the interactive law. This can be explained by a couple of factors. Firstly, during our experiments we are dealing with tail distribution, which means that spikes in the plots are usually connected with higher response times. This is because we are dealing with the distributed environment in the cloud, where network instability can cause these higher request times. Secondly, we do not know exactly how logging mechanism works in memaslap. It can be the case that the client after receiving response immediately sends a new request and after that it logs receiving of the response. What is more, data from the client can have some small error margin, especially because we are taking into account data only from the part of the running time (after stability reached and before cool down). This might be the cause of these differences. However, these discrepancies are rather small and do not undermine the validity of my experiments.

\pagebreak

\section*{Appendix - changes in the source code}

In order to measure the number of requests being actively processed in the middleware I had to enrich my source code in 2 counters - one keeping track of number of active getter workers, the other one keeping track of set requests for which response we are waiting from the servers. With these 2 counters is connected a necessary synchronization (all getter threads have access to the first counter, all setter threads have access to the second counter), which may decrease the performance of the middleware slightly.
% TODO: add that it doesn't influence it significantly 

\end{document}
